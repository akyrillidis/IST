<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!--
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
-->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:16px;
    margin-left: auto;
    margin-right: auto;
    width: 960px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }



  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<html>
<head>
	<title>On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons</title>
	<meta property="og:title" content="On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
	<span style="font-size:32px">On the Convergence of Shallow Neural Network Training with Randomly Masked Neurons</span><br>
	</center>
          <table align="center" width="960px">
            <tbody><tr>
                    <td align="center" width="240px">
              <center>
                <span style="font-size:16px"><a href="">Fangshuo (Jasper) Liao</a><sup>1</sup></span>
                </center>
                </td>                                        
		                <td align="center" width="240px">
              <center>
                <span style="font-size:16px"><a href="http://akyrillidis.github.io/">Anastasios Kyrillidis</a><sup>1</sup></span>
                </center>
                </td>
            </tr>
	
	<table align="center" width="500px">
            <tbody><tr>                    
                    <td align="center" width="250px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>Rice CS</span>
                </center>
        </tr></tbody></table>
	
		    <table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href=" "> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://openreview.net/pdf?id=e7mYYMSyZH"> [TMLR]</a>
                  </span>
                </center>
              </td>


              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./cite.txt"> [BibTeX]</a>
                  </span>
                </center>
              </td>

            </tr></tbody>
          </table>
		<br>
		<td align=center width=960px>
			<center>
			<td><img class="round" style="width:960px" src="./assets/img/theory_IST.png"/></td>
			</center>
		</td>
		    
	<br><hr>
        <center>
          <h2>
            Abstract
          </h2>
        </center>
        <p>
        <left>
          With the motive of training all the parameters of a neural network, we study why and when
one can achieve this by iteratively creating, training, and combining randomly selected subnetworks. Such scenarios have either implicitly or explicitly emerged in the recent literature:
see e.g., the Dropout family of regularization techniques, or some distributed ML training
protocols that reduce communication/computation complexities, such as the Independent
Subnet Training protocol. While these methods are studied empirically and utilized in practice, they often enjoy partial or no theoretical support, especially when applied on neural
network-based objectives.
In this manuscript, our focus is on overparameterized single hidden layer neural networks
with ReLU activations in the lazy training regime. By carefully analyzing i) the subnetworks’
neural tangent kernel, ii) the surrogate functions’ gradient, and iii) how we sample and
combine the surrogate functions, we prove linear convergence rate of the training error –up
to a neighborhood around the optimal point– for an overparameterized single-hidden layer
perceptron with a regression loss. Our analysis reveals a dependency of the size of the
neighborhood around the optimal point on the number of surrogate models and the number
of local training steps for each selected subnetwork. Moreover, the considered framework
generalizes and provides new insights on dropout training, multi-sample dropout training,
as well as Independent Subnet Training; for each case, we provide convergence results as
corollaries of our main theorem. <br> <br>
		
	  This paper is part of the IST project, ran by PIs Anastasios Kyrillidis, Chris Jermaine, and Yingyan Lin. More info <a href="https://akyrillidis.github.io/ist/">here.</a>
        </left>
      </p>
      <br>
	<hr>

	<table align=center width=960px>
		<center><h1>Introduction</h1></center>
		<tr>
			<td>
				Overparameterized neural networks have led to both unexpected empirical success in deep learning, and new
techniques in analyzing neural network training. While
theoretical work in this field has led to a diverse set of new overparameterized neural network architectures and training algorithms, most efforts fall under the
following scenario: in each iteration, we perform a gradient-based update that involves all parameters of the
neural network in both the forward and backward propagation. Yet, advances in regularization techniques, computationally-efficient 
				and communication-efficient distributed training methods favor a different narrative: one would –explicitly or implicitly– train smaller
and randomly-selected models within a large model, iteratively. This brings up the following question:
				<br>
				<br>
				<center>“Can one meaningfully train an overparameterized ML model by iteratively training
and combining together smaller versions of it?”
				</center>
				<br>
				
				<p><b>Motivation and connection to existing methods.</b></p>
				The study of partial models/subnetworks that reside in a large dense network have drawn increasing attention.
				<ul>
					<li> <em> Dropout regularization.</em> Dropout is a widely-accepted technique against overfitting in deep learning. In each training
step, a random mask is generated from some pre-defined distribution, and used to mask-out part of the
neurons in the neural network. Later variants of dropout include the drop-connect, multisample dropout , Gaussian dropout , and the variational dropout. Here, we restrict our attention to the vanilla dropout, and the multi-sample dropout.
The vanilla dropout corresponds to our framework, if in the latter we sample only one mask per iteration,
and let the subnetwork perform only one gradient descent update. The multi-sample dropout extends the
vanilla dropout in that it samples multiple masks per iteration. For regression tasks, our theoretical result
						implies convergence guarantees for these two scenarios on a single hidden-layer perceptron.</li>
						
					<li> <em>Distributed ML training.</em> Recent advances in distributed model/parallel training have led to variants of
distributed gradient descent protocols. Yet, all training parameters are updated per outer step, which could be computationally
and communication inefficient, especially in cases of high communication costs per round. The Independent
Subnetwork Training (IST) protocol goes one step further: IST splits the model vertically,
where each machine contains all layers of the neural network, but only with a (non-overlapping) subset of
neurons being active in each layer. Multiple local SGD steps can be performed without the workers having
to communicate. Methods in this line of work achieves higher communication efficiency and accuracy that is
comparable to centralized training. Yet, the theoretical
understanding of IST is currently missing. Our theoretical result implies convergence guarantees for IST for
a single hidden-layer perceptron under the simplified assumption that every worker has full data access, and
provides insights on how the number of compute nodes affects the performance of the overall protocol.</li> 
				</ul>
				
				<p><b>Contributions.</b>This paper has the following key contributions:</p>
					<ul>
  						<li>We provide convergence rate guarantees for i) dropout regularization, ii) multisample dropout, iii) and multi-worker IST, given a regression task on a single-hidden layer perceptron.</li>
  						<li>We show that the NTK of surrogate models stays close to the infinite width NTK, thus being positive definite. Consequently, our work shows that training over surrogate models still enjoys linear convergence.</li>
  						<li>For subnetworks defined by Bernoulli masks with a fixed distribution parameter, we show that aggregated gradient in the first local step is a biased estimator of the desirable gradient of the whole network, with the bias term decreasing as the number of subnetworks grows. Moreover, all aggregated gradients during local training stays close to the aggregated gradient of the first local step. This finding leads to linear convergence of the above training framework with an error term under Bernoulli masks.</li>
						<li>For masks sampled from categorical distribution, we provide tight bounds i) on the average loss increase, when sampling a subnetwork from the whole network; ii) on the loss decrease, when the independently trained subnetworks are combined into the whole model. This finding leads to linear convergence with a slightly different error term than the Bernoulli mask scenario.</li>
					</ul>		
				
				<p><b>Main statement (informal).</b> The following consitutes an informal statement out of this work; more details and results can be found in the extended version of the paper.</p>
				
				<br>
				<br>
				<center>
					Consider the training scheme shown in Figure above. If the masks are generated from a Bernoulli distribution or categorical distribution, under
sufficiently large over-parameterization coefficient, and sufficiently small learning rate, training the large
model via surrogate subnetworks still converges linearly, up to an neighborhood around the optimal point.
				</center>
				<br>
			</td>
		</tr>
	</table>
		    
	
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					AK acknowledges funding by the NSF/Intel (CNS-2003137).
					This code for this template can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

